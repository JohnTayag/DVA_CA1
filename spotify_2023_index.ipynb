{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "\n",
    "#THEORETICAL FRAMEWORK --------------------------------\n",
    "#https://www.kaggle.com/datasets/nelgiriyewithana/top-spotify-songs-2023\n",
    "\n",
    "#DATA SELECTION ----------------------------------\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir('C://Users//john8/Year 4/Data Analysis')\n",
    "\n",
    "data= pd.read_csv(\"spotify-2023.csv\", encoding='latin-1')\n",
    "\n",
    "print(data.info())\n",
    "print(data.head())\n",
    "print(data.describe())\n",
    "print(\"Number of rows:\", data.shape[0])\n",
    "\n",
    "#IMPUTATION OF MISSING DATA ----------------------------------------\n",
    "\n",
    "#1. Handling Missing Values:\n",
    "print(\"Missing values before handling:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "#in_shazam_charts        50\n",
    "#key                     95                                   \n",
    "\n",
    "# decide to drop rows where above values are null\n",
    "data.dropna(subset=['in_shazam_charts', 'key'], inplace=True)\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# decide to drop rows where values are 0\n",
    "#data = data[data != 0].dropna()\n",
    "#data = data[(data != 0).all(axis=1)]\n",
    "\n",
    "# remove rows with NaN or inf values\n",
    "#data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "#2. Outlier Detection and Treatment:\n",
    "\n",
    "integer_columns = ['artist_count', 'released_year', 'released_month', 'released_day', \n",
    "                   'in_spotify_playlists', 'in_spotify_charts', 'in_apple_playlists',\n",
    "                   'in_apple_charts', 'in_deezer_charts', 'bpm', 'danceability_%',\n",
    "                   'valence_%', 'energy_%', 'acousticness_%', 'instrumentalness_%',\n",
    "                   'liveness_%', 'speechiness_%']\n",
    "\n",
    "# Convert the selected columns to numeric data types\n",
    "data[integer_columns] = data[integer_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate the z-scores for each numeric column\n",
    "z_scores = np.abs((data[integer_columns] - data[integer_columns].mean()) / data[integer_columns].std())\n",
    "\n",
    "# Define the threshold for outliers\n",
    "threshold = 3\n",
    "\n",
    "# Identify rows where any z-score exceeds the threshold\n",
    "outliers_mask = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Print the number of rows before dropping outliers\n",
    "print(f\"Number of rows before dropping outliers: {len(data)}\")\n",
    "\n",
    "# Drop rows containing outliers\n",
    "data.drop(data.index[outliers_mask], inplace=True)\n",
    "\n",
    "# Print the number of rows dropped\n",
    "print(f\"Number of rows dropped due to outliers: {outliers_mask.sum()}\")\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(data)\n",
    "\n",
    "#3. Data Type Conversion:\n",
    "data_types = data.dtypes\n",
    "print(data_types)\n",
    "\n",
    "# Convert 'streams' column to numeric data type\n",
    "data['streams'] = pd.to_numeric(data['streams'], errors='coerce')\n",
    "\n",
    "# Convert other object columns to string data type\n",
    "object_columns = ['track_name', 'artist(s)_name', 'in_deezer_playlists', 'in_shazam_charts', 'key', 'mode']\n",
    "data[object_columns] = data[object_columns].astype(str)\n",
    "\n",
    "# Print updated data types\n",
    "print(data.dtypes)\n",
    "\n",
    "#4. Handling Duplicates:\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "#5. Feature Engineering:\n",
    "# decided to make a new column called total playlists by combining in_spotify_playlists, in_apple_playlists and in_deezer_playlists which may be useful later on\n",
    "# also decided to turn bpm into 3 categories based on number\n",
    "import numpy as np\n",
    "\n",
    "# check data types as we came across error when adding +\n",
    "print(\"Data Types using info() method:\")\n",
    "print(data[['in_spotify_playlists', 'in_apple_playlists', 'in_deezer_playlists']].info())\n",
    "\n",
    "print(\"Data Types using dtype attribute:\")\n",
    "print(\"in_spotify_playlists:\", data['in_spotify_playlists'].dtype)\n",
    "print(\"in_apple_playlists:\", data['in_apple_playlists'].dtype)\n",
    "print(\"in_deezer_playlists:\", data['in_deezer_playlists'].dtype)\n",
    "\n",
    "# change 'in_deezer_playlists' column to numeric data type from object to fix iussue\n",
    "data['in_deezer_playlists'] = pd.to_numeric(data['in_deezer_playlists'], errors='coerce')\n",
    "\n",
    "# drop if values are 0\n",
    "data = data[data['in_deezer_playlists'] != 0]\n",
    "data = data[data['in_apple_playlists'] != 0]\n",
    "data = data[data['in_spotify_playlists'] != 0]\n",
    "\n",
    "# verify data types after changing\n",
    "print(\"Data Types after Conversion:\")\n",
    "print(data[['in_deezer_playlists']].info())\n",
    "\n",
    "data['total_playlists'] = data['in_spotify_playlists'] + data['in_apple_playlists'] + data['in_deezer_playlists']\n",
    "\n",
    "# turn 'bpm' into categories (low, medium, high)\n",
    "print(\"Minimum BPM:\", data['bpm'].min())\n",
    "print(\"Maximum BPM:\", data['bpm'].max())\n",
    "\n",
    "bins = [65, 112, 159, 206]\n",
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "data['bpm_category'] = pd.cut(data['bpm'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "print(data['bpm_category'])\n",
    "\n",
    "#6. Encoding Categorical Variables:\n",
    "# change new bpm categories to numerical value\n",
    "# Define a dictionary to map labels to numerical values\n",
    "label_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "data['bpm_category_encoded'] = data['bpm_category'].map(label_mapping)\n",
    "\n",
    "data.dropna(subset=['bpm_category_encoded'], inplace=True)\n",
    "\n",
    "data['bpm_category_encoded'] = data['bpm_category_encoded'].astype(int)\n",
    "\n",
    "\n",
    "print(data[['bpm_category', 'bpm_category_encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate analysis & Feature Selection ------------------------------------------------\n",
    "print(\"Number of rows:\", data.shape[0])\n",
    "print(data.info())\n",
    "print(data.head())\n",
    "print(data.describe())\n",
    "\n",
    " #   Column                Non-Null Count  Dtype   \n",
    "#---  ------                --------------  -----   \n",
    "# 0   track_name            642 non-null    object  \n",
    "# 1   artist(s)_name        642 non-null    object  \n",
    "# 2   artist_count          642 non-null    int64   \n",
    "# 3   released_year         642 non-null    int64   \n",
    "# 4   released_month        642 non-null    int64   \n",
    "# 5   released_day          642 non-null    int64   \n",
    "# 6   in_spotify_playlists  642 non-null    int64   \n",
    "# 7   in_spotify_charts     642 non-null    int64   \n",
    "# 8   streams               642 non-null    int64   \n",
    "# 9   in_apple_playlists    642 non-null    int64   \n",
    "# 10  in_apple_charts       642 non-null    int64   \n",
    "# 11  in_deezer_playlists   614 non-null    float64\n",
    "# 12  in_deezer_charts      642 non-null    int64   \n",
    "# 13  in_shazam_charts      642 non-null    object  \n",
    "# 14  bpm                   642 non-null    int64   \n",
    "# 15  key                   642 non-null    object  \n",
    "# 16  mode                  642 non-null    object  \n",
    "# 17  danceability_%        642 non-null    int64   \n",
    "# 18  valence_%             642 non-null    int64   \n",
    "# 19  energy_%              642 non-null    int64   \n",
    "# 20  acousticness_%        642 non-null    int64   \n",
    "# 21  instrumentalness_%    642 non-null    int64   \n",
    "# 22  liveness_%            642 non-null    int64   \n",
    "# 23  speechiness_%         642 non-null    int64   \n",
    "# 24  total_playlists       614 non-null    float64 \n",
    "# 25  bpm_category          641 non-null    category\n",
    "# 26  bpm_category_encoded  641 non-null    int32\n",
    "# dtypes: category(2), float64(2), int64(18), object(5)\n",
    "\n",
    "# Correlation Coefficient of DV to IV variables\n",
    "predictor_variables = ['streams']\n",
    "response_variables = ['in_spotify_playlists',\n",
    "                      'in_spotify_charts', 'in_apple_playlists', \n",
    "                      'in_apple_charts', 'in_deezer_playlists', 'in_deezer_charts', 'in_shazam_charts', \n",
    "                      'artist_count', 'released_day', 'released_year', 'released_month', 'bpm', 'danceability_%', 'valence_%', 'energy_%', 'acousticness_%', 'instrumentalness_%',\n",
    "                       'liveness_%', 'speechiness_%', 'total_playlists', 'bpm_category_encoded']\n",
    "\n",
    "# change columns to integer type based on values above\n",
    "data = data.dropna(subset=['in_deezer_playlists'])\n",
    "data['in_shazam_charts'] = data['in_deezer_playlists'].astype(int)\n",
    "data['total_playlists'] = data['total_playlists'].astype(int)\n",
    "\n",
    "for predictor_var in predictor_variables:\n",
    "    for response_var in response_variables:\n",
    "        corr_coef, p_value = pearsonr(data[predictor_var], data[response_var])\n",
    "        print(f\"Pearson correlation coefficient between '{predictor_var}' and '{response_var}': {corr_coef:.2f}\")\n",
    "\n",
    "# Scatterplot of DV to IV variables\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictor_var = 'streams'\n",
    "\n",
    "for response_var in response_variables:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data[predictor_var], data[response_var], alpha=0.5, label='Data points')\n",
    "    plt.title(f\"{response_var} vs {predictor_var}\")\n",
    "    plt.xlabel(predictor_var)\n",
    "    plt.ylabel(response_var)\n",
    "    \n",
    "    # fit a linear regression line\n",
    "    m, b = np.polyfit(data[predictor_var], data[response_var], 1)\n",
    "    plt.plot(data[predictor_var], m * data[predictor_var] + b, color='red', label='Regression line')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Summary of Correlation Coefficient of DV(streams) to IV variables\n",
    "\n",
    "#Strong positive correlations (≥ 0.5):\n",
    "#'in_spotify_playlists': 0.77\n",
    "#'in_shazam_charts': 0.70\n",
    "#'in_deezer_playlists': 0.70\n",
    "\n",
    "#Moderate positive correlations (0.3 ≤ r < 0.5):\n",
    "#'in_apple_playlists': 0.63\n",
    "#'total_playlists': 0.78\n",
    "\n",
    "#Weak positive correlations (0.1 ≤ r < 0.3):\n",
    "#'released_month': 0.02\n",
    "#'acousticness_%': 0.01\n",
    "#'instrumentalness_%': 0.01\n",
    "\n",
    "#Weak negative correlations (-0.3 ≤ r < -0.1):\n",
    "#'artist_count': -0.11\n",
    "#'released_day': 0.06\n",
    "#'released_year': -0.39\n",
    "#'danceability_%': -0.10\n",
    "#'valence_%': -0.10\n",
    "#'energy_%': -0.07\n",
    "#'speechiness_%': -0.04\n",
    "#'bpm_category_encoded': 0.01\n",
    "\n",
    "# IV to IV scatterplots and correlations want weak or low correlation\n",
    "correlation_matrix_all = data[response_variables].corr()\n",
    "\n",
    "# Plot heatmap of correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix_all, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"Correlation Matrix of Response Variables\")\n",
    "plt.show()\n",
    "\n",
    "# Summary of Correlation Coefficient of IV to IV variables\n",
    "# in_spotify_playlist is highly correlated (0.77) with in_deezer_playlists & in_shazam_charts - top 3 major song platforms ->\n",
    "# if a song is on a spotify playlist then theres 77% chance its in a deezer playlist OR shazam charts\n",
    "# opposite is if its either in a deezer playlist OR shazam chart THEN theres 77% chance its in a spotify playlist\n",
    "# UNEXPECTED 100% of in_deezer_playlists & in_shazam_charts - meaning if a song is in either one then it exists in the other one\n",
    "\n",
    "# Investigate 100% collinearity of in_deezer_playlists & in_shazam_charts\n",
    "#1. Check Unique Values: Verify if the values in both columns are identical for all observations. This can help confirm the observed collinearity.\n",
    "\n",
    "unique_deezer = data['in_deezer_playlists'].unique()\n",
    "unique_shazam = data['in_shazam_charts'].unique()\n",
    "\n",
    "print(\"Unique values in 'in_deezer_playlists':\", unique_deezer)\n",
    "print(\"Unique values in 'in_shazam_charts':\", unique_shazam)\n",
    "\n",
    "if len(unique_deezer) == 1 and len(unique_shazam) == 1 and unique_deezer[0] == unique_shazam[0]:\n",
    "    print(\"The unique values in 'in_deezer_playlists' and 'in_shazam_charts' are identical for all observations.\")\n",
    "else:\n",
    "    print(\"The unique values in 'in_deezer_playlists' and 'in_shazam_charts' are not identical for all observations.\")\n",
    "\n",
    "\n",
    "#2. Cross-Tabulation: Create a cross-tabulation (contingency table) between the two variables to see the frequency distribution of their values. This can provide additional insights into the relationship between the variables.\n",
    "cross_tab = pd.crosstab(data['in_deezer_playlists'], data['in_shazam_charts'])\n",
    "\n",
    "print(\"Cross-Tabulation (Contingency Table):\")\n",
    "print(cross_tab)\n",
    "#In this case, the contingency table reveals that there is a one-to-one correspondence between the values of 'in_deezer_playlists' and \n",
    "#'in_shazam_charts'. Each unique value of 'in_deezer_playlists' corresponds to a unique value of 'in_shazam_charts', and vice versa.\n",
    "#From a data analysis perspective, this indicates a perfect association between the two variables. In other words, whenever a song appears in \n",
    "#'in_deezer_playlists', it also appears in 'in_shazam_charts', and vice versa. \n",
    "#This could suggest either a strong relationship between these two variables or potential redundancy, as they may convey similar information.\n",
    "\n",
    "# REVIEW AND CONCLUSION\n",
    "# drop weak correlation values of DV to IV variables -> released_month, acousticness_%, instrumentalness_%, artist_count, released_day, released_year, danceability_%, valence_%, energy_%, speechiness_%\n",
    "# drop in_shazam_charts as multi collinearity exists between response variables\n",
    "# so, although theres 88% collinearity of in_spotify_playlist & in_deezer_playlists - decide to keep in_deezer_playlists as it makes sense and needed for predicting streams\n",
    "\n",
    "# KEPT VALUES for further analysis and modelling\n",
    "# predictor - streams\n",
    "# response - in_spotify_playlists, in_deezer_playlists, in_apple_playlists, total_playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization & Standardization ----------------------------------\n",
    "\n",
    "#https://www.digitalocean.com/community/tutorials/normalize-data-in-python\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_response_variables = ['in_spotify_playlists',\n",
    "                      'in_spotify_charts', 'in_apple_playlists', \n",
    "                      'in_apple_charts', 'in_deezer_playlists', 'in_deezer_charts', 'in_shazam_charts', \n",
    "                      'artist_count', 'released_day', 'released_year', 'released_month', 'bpm', 'danceability_%', 'valence_%', 'energy_%', 'acousticness_%', 'instrumentalness_%',\n",
    "                       'liveness_%', 'speechiness_%', 'total_playlists']\n",
    "\n",
    "normalized_variables = ['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'total_playlists', 'streams']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "data[normalized_variables].boxplot()\n",
    "plt.title('Box Plot of Response Variables')\n",
    "plt.ylabel('Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# clear from boxplots that we need to scale and normalize our data, also drop rows where in_spotify_playlist > 15000 as its an outlier\n",
    "data = data[data['in_spotify_playlists'] < 15000]\n",
    "data = data[data['in_deezer_playlists'] < 600]\n",
    "\n",
    "# Normalize the response variables directly in the original DataFrame\n",
    "data[normalized_variables] = preprocessing.normalize(data[normalized_variables], axis=0)\n",
    "\n",
    "# Check if data is normalized\n",
    "magnitudes = np.linalg.norm(data[normalized_variables], axis=0)\n",
    "\n",
    "# Print the magnitudes after normalization\n",
    "for var, magnitude in zip(normalized_variables, magnitudes):\n",
    "    print(f\"Magnitude of '{var}' after normalization: {magnitude:.4f}\")\n",
    "\n",
    "# Visualize boxplots of response variables after normalization\n",
    "plt.figure(figsize=(10, 6))\n",
    "data[normalized_variables].boxplot()\n",
    "plt.title('Box Plot of Response Variables (After Normalization)')\n",
    "plt.ylabel('Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#1. Standardization (Z-score normalization):\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data[normalized_variables])\n",
    "data_standardized = pd.DataFrame(data_standardized, columns=normalized_variables)\n",
    "print(data_standardized.head())\n",
    "\n",
    "#2. Min-Max Scaling:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[normalized_variables] = scaler.fit_transform(data[normalized_variables])\n",
    "print(\"Minimum values after Min-Max scaling:\")\n",
    "print(data[normalized_variables].min())\n",
    "print(\"\\nMaximum values after Min-Max scaling:\")\n",
    "print(data[normalized_variables].max())\n",
    "\n",
    "#3. Robust Scaling:\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "data[normalized_variables] = robust_scaler.fit_transform(data[normalized_variables])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Modelling ----------------------------\n",
    "\n",
    "  #Conduct simple linear regressions for each IV/DV pair\n",
    "   # Use the non-redundant independent variables in the analysis to find the best fitting model - use all your different types scaled data for variables to find best model\n",
    "  # go with one variavle banner then two then three until you find best model\n",
    "  #best model shows highest r squared mse and ssr\n",
    "    # Use the best fitting model to make predictions about the dependent variable\n",
    "\n",
    "# First regression analysis with 'in_spotify_playlists' predicting 'streams'\n",
    "import statsmodels.api as sm\n",
    "all_data = sm.add_constant(data['in_spotify_playlists'])\n",
    "model = sm.OLS(data['streams'], all_data)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "# Line below is for the standard error\n",
    "print(result.scale**0.5)\n",
    "\n",
    "# Second regression analysis with both 'in_spotify_playlists', 'in_spotify_charts' & 'in_deezer_playlists' predicting 'streams'\n",
    "data2 = sm.add_constant(data[['in_spotify_playlists', 'in_deezer_playlists']])\n",
    "model2 = sm.OLS(data['streams'], data2)\n",
    "result2 = model2.fit()\n",
    "print(result2.summary())\n",
    "# Standard error\n",
    "print(result2.scale**0.5)\n",
    "print(result2.ssr)\n",
    "\n",
    "# Third regression analysis with both 'in_spotify_playlists', 'in_spotify_charts', 'in_deezer_playlists' & 'in_apple_playlists' predicting 'streams'\n",
    "data3 = sm.add_constant(data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists']])\n",
    "model3 = sm.OLS(data['streams'], data3)\n",
    "result3 = model3.fit()\n",
    "print(result3.summary())\n",
    "# Standard error\n",
    "print(result3.scale**0.5)\n",
    "print(result3.ssr)\n",
    "\n",
    "# Fourth regression analysis with both 'in_spotify_playlists', 'in_spotify_charts', 'in_deezer_playlists', 'in_apple_playlists' & 'total_playlists' predicting 'streams'\n",
    "data4 = sm.add_constant(data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'total_playlists']])\n",
    "model4 = sm.OLS(data['streams'], data4)\n",
    "result4 = model4.fit()\n",
    "print(result4.summary())\n",
    "# Standard error\n",
    "print(result4.scale**0.5)\n",
    "print(result4.ssr)\n",
    "\n",
    "# Interpretation:\n",
    "# The model has a moderate-to-high R-squared value of 0.631, indicating that about 63.1% of the variance in the dependent variable (streams) is explained by the independent variables.\n",
    "# The F-statistic is highly significant (p-value < 0.001), suggesting that the overall regression model is statistically significant.\n",
    "# The p-values for 'in_spotify_playlists', 'in_deezer_playlists', and 'total_playlists' are all significant (p-value < 0.05), indicating that these variables are likely to have a significant linear relationship with the dependent variable.\n",
    "# However, 'in_apple_playlists' has a high p-value (p-value = 0.441), indicating that it may not be a significant predictor in the model.\n",
    "# The AIC and BIC values are 1030 and 1052, respectively, suggesting that this model is relatively better than other models with higher AIC and BIC values.\n",
    "# The Condition Number (Cond. No.) is very high, indicating potential multicollinearity issues or singularity in the design matrix.\n",
    "\n",
    "# Conclusion:\n",
    "# Overall, the model appears to be statistically significant, with most coefficients being significant predictors of the dependent variable. \n",
    "# However, the high Condition Number suggests potential multicollinearity issues that should be investigated further. \n",
    "# Additionally, the coefficient for 'in_apple_playlists' is not significant, indicating that it may not contribute much to predicting the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normalized_variables = ['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'total_playlists', 'streams']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data[normalized_variables])\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Get the principal components\n",
    "pca_data = pca.transform(scaled_data)\n",
    "\n",
    "# Percentage of variance explained by each principal component\n",
    "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "\n",
    "# Plot scree plot\n",
    "labels = ['PC' + str(i) for i in range(1, len(per_var) + 1)]\n",
    "plt.bar(x=range(1, len(per_var) + 1), height=per_var, tick_label=labels)\n",
    "plt.ylabel(\"Percentage of Explained Variance\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# Plot PCA graph\n",
    "pca_df = pd.DataFrame(pca_data, columns=labels)\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title(\"PCA graph\")\n",
    "plt.xlabel(f'PC1 {per_var[0]}%')\n",
    "plt.ylabel(f'PC2 {per_var[1]}%')\n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    "plt.show()\n",
    "\n",
    "# Get loading scores for each variable\n",
    "loading_scores = pd.Series(pca.components_[0], index=normalized_variables)\n",
    "\n",
    "# Get top 10 variables contributing most to the principal component\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    "top_10_variables = sorted_loading_scores[:10]\n",
    "\n",
    "# Plot top 10 variables\n",
    "plt.bar(top_10_variables.index, top_10_variables.values)\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Loading Score')\n",
    "plt.title('Top 10 Variables Contributing to First Principal Component')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# In conclusion, Principal Component Analysis (PCA) has provided valuable insights into our dataset containing variables related to \n",
    "# music streaming platforms. Through standardization and PCA, we transformed our original variables into principal components, \n",
    "# which allowed us to identify patterns and reduce the dimensionality of the data. The scree plot helped us understand the variance \n",
    "# explained by each principal component, aiding in the decision-making process regarding the number of components to retain. \n",
    "# The PCA graph visually depicted the distribution of data points in the reduced-dimensional space, offering a glimpse into potential \n",
    "# clusters or patterns. Additionally, analyzing the loading scores helped us identify the variables contributing most significantly to \n",
    "# each principal component, providing valuable information about the underlying structure of the data. Overall, PCA has proven to be a \n",
    "# powerful tool for exploring and understanding complex datasets, offering a deeper understanding of the relationships and dynamics within the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis - Linkage ------------------------------------\n",
    "import seaborn as sb\n",
    "def show_boxplot(df):\n",
    "    plt.rcParams['figure.figsize'] = [14, 6]\n",
    "    sb.boxplot(data=df, orient='v')\n",
    "    plt.title(\"Outliers Distribution\", fontsize=16)\n",
    "    plt.ylabel(\"Range\", fontweight='bold')\n",
    "    plt.xlabel(\"Attributes\", fontweight='bold')\n",
    "\n",
    "cleaned_data = data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'total_playlists', 'streams']]\n",
    "cleaned_data.info()\n",
    "\n",
    "show_boxplot(cleaned_data)\n",
    "\n",
    "def remove_outliers(data):\n",
    "    df = data.copy()\n",
    "    for col in list(df.columns):\n",
    "        Q1 = df[str(col)].quantile(0.25)\n",
    "        Q3 = df[str(col)].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5*IQR\n",
    "        upper_bound = Q3 + 1.5*IQR\n",
    "        df = df[(df[str(col)] >= lower_bound) & (df[str(col)] <= upper_bound)]\n",
    "        return df\n",
    "    \n",
    "without_outliers = remove_outliers(cleaned_data)\n",
    "show_boxplot(without_outliers)\n",
    "without_outliers.shape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_scaler = StandardScaler()\n",
    "scaled_data = data_scaler.fit_transform(without_outliers)\n",
    "scaled_data.shape\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cut_tree\n",
    "\n",
    "complete_clustering = linkage(scaled_data, method = \"complete\", metric=\"euclidean\")\n",
    "average_clustering = linkage(scaled_data, method = \"average\", metric=\"euclidean\")\n",
    "single_clustering = linkage(scaled_data, method = \"single\", metric=\"euclidean\")\n",
    "\n",
    "dendrogram(complete_clustering)\n",
    "plt.show()\n",
    "\n",
    "# Conclusion\n",
    "# Using elbow method complete clustering looks to be the best graph to cluster our data\n",
    "# There seems to be around 4 to 5 clusters\n",
    "\n",
    "# The \"elbow method\" is a common heuristic used in dendrogram analysis to determine the optimal number of clusters. \n",
    "# The idea is to look for the point where the slope of the dendrogram changes significantly, which is often referred to as the \"elbow.\"\n",
    "\n",
    "# To use the elbow method, you can plot the dendrogram and look for the point where the branches start to become more horizontal. \n",
    "# This point represents the optimal number of clusters because it indicates that adding more clusters would not significantly improve \n",
    "# the clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis - SK Learning --------------------------------\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "km = KMeans(n_clusters=4, n_init='auto')\n",
    "\n",
    "y_predicted = km.fit_predict(data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'streams']])\n",
    "y_predicted\n",
    "\n",
    "data['cluster'] = y_predicted\n",
    "data.head()\n",
    "\n",
    "df1 = data[data.cluster ==0]\n",
    "df2 = data[data.cluster ==1]\n",
    "df3 = data[data.cluster ==2]\n",
    "df4 = data[data.cluster ==3]\n",
    "plt.scatter(df1.in_spotify_playlists, df1['streams'], color='green')\n",
    "plt.scatter(df2.in_deezer_playlists, df2['streams'], color='red')\n",
    "plt.scatter(df3.in_apple_playlists, df3['streams'], color='black')\n",
    "plt.scatter(df4.total_playlists, df4['streams'], color='yellow')\n",
    "plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], color='purple', marker='*', label='centroid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis - DBSCAN -----------------------\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare your data\n",
    "X = data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'streams']].values\n",
    "\n",
    "# Define and fit the DBSCAN model\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "\n",
    "# Extract cluster labels and identify core samples\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Determine the number of clusters\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Visualize the clusters\n",
    "unique_labels = set(labels)\n",
    "colours = ['y', 'b', 'g', 'r']\n",
    "for k, col in zip(unique_labels, colours):\n",
    "    if k == -1:\n",
    "        col = 'k'  # Black used for noise\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title(f\"Number of clusters: {n_clusters}\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# if we include total_playlists it predicts theres 1 cluster, if we remove it predicts we have 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing ---------------------------------\n",
    "\n",
    "# Step 1: Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "normalized_data = MinMaxScaler().fit_transform(data[['in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists']])\n",
    "\n",
    "# Step 2: Define weights\n",
    "\n",
    "# Summary of Correlation Coefficient of DV(streams) to IV variables\n",
    "#Strong positive correlations (≥ 0.5):\n",
    "#'in_spotify_playlists': 0.77\n",
    "#'in_deezer_playlists': 0.70\n",
    "#Moderate positive correlations (0.3 ≤ r < 0.5):\n",
    "#'in_apple_playlists': 0.63\n",
    "\n",
    "# based on the values from multivariate analysis results we adjust the weights accordingly\n",
    "weights = [0.40, 0.35, 0.25]\n",
    "\n",
    "# Step 3: Calculate Composite Index\n",
    "composite_index = np.dot(normalized_data, weights)\n",
    "\n",
    "# Step 4: Rank the rows based on composite index\n",
    "data['Composite_Index'] = composite_index\n",
    "top_10_songs = data.sort_values(by='Composite_Index', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 songs with track name and artist name\n",
    "print(top_10_songs[['track_name', 'artist(s)_name', 'in_spotify_playlists', 'in_deezer_playlists', 'in_apple_playlists', 'streams']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation if Index ------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "top_10_songs = top_10_songs.sort_values(by='Composite_Index', ascending=False)  # Sort by descending index\n",
    "song_names = top_10_songs['track_name'].tolist()\n",
    "artists = top_10_songs['artist(s)_name'].tolist()\n",
    "composite_indices = top_10_songs['Composite_Index'].tolist()\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(4, 6))  # Adjust figure size for better readability\n",
    "bars = plt.bar(range(1, len(song_names) + 1), composite_indices, color='skyblue')\n",
    "\n",
    "# Add song names and artists as labels inside the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - .462, f'{song_names[i]} by {artists[i]}', ha='center', va='center', rotation=90)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Ranking')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.title('Top 10 Songs of 2023 Based on Playlists Appearance')\n",
    "plt.xticks(range(1, len(song_names) + 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
